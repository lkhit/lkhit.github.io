<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Research on Autonomous Driving</title>
      <link href="/2018/03/29/TORCS/"/>
      <url>/2018/03/29/TORCS/</url>
      <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Deep Reinforcement learning (RL) is considered to be a strong machine learning algorithm which can be used to teach machines through interaction with the environment and learning from trial-error. However, the real-world application of RL is still restricted by a variety of reasons. The two most significant challenges of RL are the large exploration space and the difficulty to converge. Training RL agent with expert demonstration is technically an interesting way to accelerate the exploration process and increase the stability. In this work, we propose a continuous reinforcement learning method which combines Deep Deterministic Policy Gradient (DDPG) with expert demonstrations for intelligent decision making for autonomous driving. The proposed algorithm introduces supervised classification loss function for updating the actor networks. In addition, reward construction is combined to make the training process more stable and efficient. The proposed algorithm is tested on a popular autonomous driving simulator called TORCS. The experimental results show that our method not only improves training efficiency, but also improves learning stability.</p><p>The autonomous driving task can be viewed as figure1.</p><p><img src="http://i1.bvimg.com/638842/db51ca0b3c8a93d0.png" alt="example image" title="figure1.autonomous driving framework"></p><p>The training track and testing track are showing as figure2 and figure3.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i1.bvimg.com/638842/71221fdeccc3d4c8.png" height="200" width="250"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i1.bvimg.com/638842/254e598739862178.png" height="200" width="250"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i1.bvimg.com/638842/254e598739862178.png" height="200" width="250"></div></div></div></div><p>The training process is showing as figure4.<br><img src="http://i1.bvimg.com/638842/916c3fa11e7644bf.png" alt="example image" title="figure4.training process"></p><p>We visualize the training process and test process, the results are showing as figure5 and figure6.<br><img src="http://i4.bvimg.com/638842/df006d34d802b64b.gif" alt="example image" title="figure5.training process"></p><p><img src="http://i4.bvimg.com/638842/12d57afcac78e3b0.gif" alt="gif example." title="figure6.test process"></p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>Research</title>
      <link href="/2018/03/29/%E5%8D%BF%E5%8D%BF/"/>
      <url>/2018/03/29/%E5%8D%BF%E5%8D%BF/</url>
      <content type="html"><![CDATA[<h1 id="This-is-a-test"><a href="#This-is-a-test" class="headerlink" title="This is a test!"></a>This is a test!</h1>]]></content>
      
      
    </entry>
    
  
  
</search>
