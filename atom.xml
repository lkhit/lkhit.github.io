<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Lucky&#39;s hoom</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-04-11T11:37:54.893Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Liu Kai</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Daily Notes</title>
    <link href="http://yoursite.com/2018/04/03/Daily%20Notes/"/>
    <id>http://yoursite.com/2018/04/03/Daily Notes/</id>
    <published>2018-04-03T06:35:25.000Z</published>
    <updated>2018-04-11T11:37:54.893Z</updated>
    
    <content type="html"><![CDATA[<font size="5"> <strong>LaTex usage:</strong> </font><p><strong>Convert png to eps:</strong></p><ul><li><p>method1 :<br>open the terminal in the current file, and run the command:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bmeps -c original.png original.eps</span><br></pre></td></tr></table></figure></li><li><p>method2:<br>online transfer link <a href="http://www.tlhiv.org/rast2vec/" target="_blank" rel="noopener">http://www.tlhiv.org/rast2vec/</a></p></li></ul><hr><p><strong>Draw trackPos on the CG1 track</strong></p><ul><li>One of the sensors that TORCS provide us is (Track_Pos) sensor which is defined by: The Distance between the car and the track axis. This value is normalized with respect to the track width: it is 0 when car is on the axis, -1 when the car is on the right edge of the track and +1 when it is on the left edge of the car. </li><li>I have depended on saving this values for: straight line, right curves, and left curves. In addition to that, I have saved the image of the car when moving on this track in order to make sure that these track_pos values are correct. Then, I have projected the track_pos values on the image of the track, and finally I have these two images.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;font size=&quot;5&quot;&gt; &lt;strong&gt;LaTex usage:&lt;/strong&gt; &lt;/font&gt;

&lt;p&gt;&lt;strong&gt;Convert png to eps:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;method1 :&lt;br&gt;open the termina
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>A Deep Reinforcement Learning Algorithm with Expert Demonstrations and Supervised Loss and its application into Autonomous Driving</title>
    <link href="http://yoursite.com/2018/03/29/TORCS/"/>
    <id>http://yoursite.com/2018/03/29/TORCS/</id>
    <published>2018-03-29T03:35:25.000Z</published>
    <updated>2018-04-15T14:05:05.826Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Deep Reinforcement learning (DRL) is considered to be a strong machine learning algorithm which can be used to teach machines through interaction with the environment and learning from trial-error. However, the application of RL is still restricted by a variety of reasons. The two most significant challenges of RL are the large exploration space and the difficulty to converge. Training RL agent with expert demonstrations is technically an interesting way to accelerate the exploration process and increase the stability. In this paper, we propose a continuous reinforcement learning method which combines Deep Deterministic Policy Gradient (DDPG) with expert demonstrations for intelligent decision making for autonomous driving. A supervised classification loss function is introduced in the algorithm to update the actor networks. In addition, reward construction is combined to make the training process more stable and efficient. The proposed algorithm is applied to a popular autonomous driving simulator called TORCS. The experimental results show that the training efficiency and learning stability are improved by utilizing our algorithm in autonomous driving.</p><h1 id="Experiment-Videos"><a href="#Experiment-Videos" class="headerlink" title="Experiment Videos"></a>Experiment Videos</h1><p>The autonomous driving mission can be viewed as Fig.1.<br><img src="http://t1.aixinxi.net/o_1cai98safo25ro54ai9dm1g90a.png-w.jpg" alt="example image"></p><p align="center"><strong>Fig.1: Autonomous driving framework</strong></p><p>The training track and testing track are shown as Figure2.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://t1.aixinxi.net/o_1cai9b3kba39t7ivo15fd1bmka.png-w.jpg" height="300" width="350"></div></div></div></div><p align="center"><strong>Figure2. Training track(left) and test track(right)</strong></p><p>We record the training video of original DDPG algorithm in 200, 500, 1000 episode respectively, and the video is shown as Figure3.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9d40kj4r20i1jn0d2fla.gif-w.jpg" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9dmq318pcql61f7u1h9n8aea.gif-w.jpg" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9ehua4vhmrh7sjaj22f7a.gif-w.jpg" height="250" width="300"></div></div></div></div><p align="center"><strong>Figure3. Training video of original DDPG</strong></p><p>For comparison experiment, we record the training video of DDPG with Expert Demonstrations algorithm in 200, 500, 1000 episode at the same time, and the video is shown as Figure4.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9jcjp1vkt1epjc451mal1qpha.gif-w.jpg" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9jtj51nq1ickkuf3fq1m9ma.gif-w.jpg" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9kh2g16i03cj10q1mtt7qba.gif-w.jpg" height="250" width="300"></div></div></div></div><p align="center"><strong>Figure4. Training video of DDPG with Expert Demonstrations</strong></p><p>Meanwhile, we record the training video of DDPG with modified reward function in 200, 500, 1000 episode, and the video is shown as Figure5.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9l5du1gm716pllvo18om92ra.gif-w.jpg" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9n1bu19e210oa1rn51po3vv5a.gif-w.jpg" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9nki5176eo64320mi31td6a.gif-w.jpg" height="250" width="300"></div></div></div></div><p align="center"><strong>Figure5. Training video of DDPG with modifed reward</strong></p><p>We visualize the test process(CG1 and CG2 track), the results are shown as figure6.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://t1.aixinxi.net/o_1cai9oeue1i5g1t49oit3e11bfa.gif-w.jpg" height="300" width="350"></div><div class="group-picture-column" style="width: 50%;"><img src="http://t1.aixinxi.net/o_1cai9pg2qved15gpv5ve611an8a.gif-w.jpg" height="300" width="350"></div></div></div></div><p align="center"><strong>Figure6. Agent test video in CG1 and CG2 track</strong></p><h1 id="Experiment-Detials"><a href="#Experiment-Detials" class="headerlink" title="Experiment Detials"></a>Experiment Detials</h1><p>More comparison detials of our algorithm are shown as Figure7, Figure8, Figure9.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9qjjmns21pmj1cfo1knbk6fa.png-w.jpg" height="180" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9rho41ori5qqisrlnm1b1ca.png-w.jpg" height="180" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9s0js18o019er16i8nhc129pa.png-w.jpg" height="180" width="300"></div></div></div></div><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1caia6q6fsvs993ttfo09ulfa.png-w.jpg" height="180" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9shii1d4ll231gqe6n719kla.png-w.jpg" height="180" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9u8ks169k191mrs0an9dmfa.png-w.jpg" height="180" width="300"></div></div></div></div><p align="center"><strong>Figure7. Reawrd detials of our algorithm</strong></p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://t1.aixinxi.net/o_1cai9us9e1hg61mhoq55nrn58sa.png-w.jpg" height="180" width="800"></div></div></div></div><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://t1.aixinxi.net/o_1cai9v9a610id1gh1fas1k8rgj6a.png-w.jpg" height="180" width="800"></div></div></div></div><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://t1.aixinxi.net/o_1cai9vmq3m2pt3812h41510a7na.png-w.jpg" height="180" width="800"></div></div></div></div><p align="center"><strong>Figure8. Cumulative steps detials of our algorithm</strong></p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 25%;"><img src="http://t1.aixinxi.net/o_1caia0epj1s7r136p1o59ffg1fv4a.png-w.jpg" height="160" width="245"></div><div class="group-picture-column" style="width: 25%;"><img src="http://t1.aixinxi.net/o_1caia14q9deekmi1a4d1mu41j92a.png-w.jpg" height="160" width="245"></div><div class="group-picture-column" style="width: 25%;"><img src="http://t1.aixinxi.net/o_1caia1kuh1rae1gio13e4vvr8jha.png-w.jpg" height="160" width="245"></div><div class="group-picture-column" style="width: 25%;"><img src="http://t1.aixinxi.net/o_1caia2eik7fp1gnb1bathhe1mn4a.png-w.jpg" height="160" width="245"></div></div></div></div><p align="center"><strong>Figure9. Velocity and trackpos detials of our algorithm</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h1&gt;&lt;p&gt;Deep Reinforcement learning (DRL) is considere
      
    
    </summary>
    
    
  </entry>
  
</feed>
