<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Lucky&#39;s hoom</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-04-19T11:15:22.944Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Liu Kai</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Daily Notes</title>
    <link href="http://yoursite.com/2018/04/03/Daily%20Notes/"/>
    <id>http://yoursite.com/2018/04/03/Daily Notes/</id>
    <published>2018-04-03T06:35:25.000Z</published>
    <updated>2018-04-19T11:15:22.944Z</updated>
    
    <content type="html"><![CDATA[<font size="5"> <strong>LaTex usage:</strong> </font><p><strong>Convert png to eps:</strong></p><ul><li><p>method1 :<br>open the terminal in the current file, and run the command:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bmeps -c original.png original.eps</span><br></pre></td></tr></table></figure></li><li><p>method2:<br>online transfer link <a href="http://www.tlhiv.org/rast2vec/" target="_blank" rel="noopener">http://www.tlhiv.org/rast2vec/</a></p></li></ul><hr><p><strong>Draw trackPos on the CG1 track</strong></p><ul><li>One of the sensors that TORCS provide us is (Track_Pos) sensor which is defined by: The Distance between the car and the track axis. This value is normalized with respect to the track width: it is 0 when car is on the axis, -1 when the car is on the right edge of the track and +1 when it is on the left edge of the car. </li><li>I have depended on saving this values for: straight line, right curves, and left curves. In addition to that, I have saved the image of the car when moving on this track in order to make sure that these track_pos values are correct. Then, I have projected the track_pos values on the image of the track, and finally I have these two images.</li></ul><p><strong>theme config:</strong></p><p><a href="http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html" target="_blank" rel="noopener">next theme config</a></p><hr><p><strong>Tranfer images to videos</strong><br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -f image2 -framerate <span class="number">25</span> -i /home/luck/<span class="number">0</span>-code/<span class="number">0</span>-research/track/data/fig/%<span class="number">03</span>d.jpg -s <span class="number">612</span>x812  foo.avi</span><br></pre></td></tr></table></figure></p><hr><p><strong>Tranfer video to gif</strong></p><p><a href="https://ezgif.com/video-to-gif" target="_blank" rel="noopener">transfer video to gif</a></p><hr><p><strong>tuchuang</strong></p><p><a href="https://tu.aixinxi.net/" target="_blank" rel="noopener">aixinxi</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;font size=&quot;5&quot;&gt; &lt;strong&gt;LaTex usage:&lt;/strong&gt; &lt;/font&gt;

&lt;p&gt;&lt;strong&gt;Convert png to eps:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;method1 :&lt;br&gt;open the termina
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>A Deep Reinforcement Learning Algorithm with Expert Demonstrations and Supervised Loss and its application into Autonomous Driving</title>
    <link href="http://yoursite.com/2018/03/29/TORCS/"/>
    <id>http://yoursite.com/2018/03/29/TORCS/</id>
    <published>2018-03-29T03:35:25.000Z</published>
    <updated>2018-04-20T08:20:34.733Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>In this paper, we propose a deep reinforcement learning(DRL) algorithm which combines Deep Deterministic Policy Gradient (DDPG) with expert demonstrations for intelligent decision making for autonomous driving. Training DRL agent with supervised learning is adopted to accelerate the exploration process and increase the stability. A supervised classification loss function is introduced in the algorithm to update the actor networks. In addition, reward construction is combined to make the training process more stable and efficient. The proposed algorithm is applied to a popular autonomous driving simulator called TORCS. The experimental results show that the training efficiency and learning stability are improved by utilizing our algorithm in autonomous driving.</p><h1 id="Experiment-Videos"><a href="#Experiment-Videos" class="headerlink" title="Experiment Videos"></a>Experiment Videos</h1><p>The autonomous driving mission can be viewed as Fig.1.<br><img src="http://t1.aixinxi.net/o_1cai98safo25ro54ai9dm1g90a.png-w.jpg" alt="example image"></p><p align="center"><strong>Fig.1: Autonomous driving framework</strong></p><p>The training track and test track are shown as Fig.2.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://t1.aixinxi.net/o_1cai9b3kba39t7ivo15fd1bmka.png-w.jpg" height="300" width="350"></div></div></div></div><p align="center"><strong>Fig.2. Training track(left) and test track(right)</strong></p><p>The training process of original DDPG algorithm is recorded in 200, 500, 1000 episode respectively, and the video is shown as Fig.3.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9d40kj4r20i1jn0d2fla.gif-w.jpg" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9dmq318pcql61f7u1h9n8aea.gif-w.jpg" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9ehua4vhmrh7sjaj22f7a.gif-w.jpg" height="250" width="300"></div></div></div></div><p align="center"><strong>Fig.3. Training video of original DDPG</strong></p><p>In addition, we record the training video of the proposed modified DDPG in 200, 500, 1000 episode at the same time, and the video is shown as Fig.4.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9jcjp1vkt1epjc451mal1qpha.gif-w.jpg" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9jtj51nq1ickkuf3fq1m9ma.gif-w.jpg" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9kh2g16i03cj10q1mtt7qba.gif-w.jpg" height="250" width="300"></div></div></div></div><p align="center"><strong>Fig.4. Training video of the proposed modifed DDPG</strong></p><p>Meanwhile, the training video that agent adopt modifed reward function by utilizing DDPG is recorded in 200, 500, 1000 episode, and the video is shown as Fig.5.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9l5du1gm716pllvo18om92ra.gif-w.jpg" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9n1bu19e210oa1rn51po3vv5a.gif-w.jpg" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9nki5176eo64320mi31td6a.gif-w.jpg" height="250" width="300"></div></div></div></div><p align="center"><strong>Fig.5. Training video of agent adopt modifed reward</strong></p><p>We visualize the test process(CG1 and CG2 track), the results are shown as Fig.6.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://t1.aixinxi.net/o_1cai9oeue1i5g1t49oit3e11bfa.gif-w.jpg" height="300" width="350"></div><div class="group-picture-column" style="width: 50%;"><img src="http://t1.aixinxi.net/o_1cai9pg2qved15gpv5ve611an8a.gif-w.jpg" height="300" width="350"></div></div></div></div><p align="center"><strong>Fig.6. Test video of agent in CG1 and CG2 track</strong></p><h1 id="More-Detials"><a href="#More-Detials" class="headerlink" title="More Detials"></a>More Detials</h1><p>More comparison detials of our proposed algorithm are shown as Fig.7, Fig.8, Fig.9.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9qjjmns21pmj1cfo1knbk6fa.png-w.jpg" height="180" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9rho41ori5qqisrlnm1b1ca.png-w.jpg" height="180" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9s0js18o019er16i8nhc129pa.png-w.jpg" height="180" width="300"></div></div></div></div><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1caia6q6fsvs993ttfo09ulfa.png-w.jpg" height="180" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9shii1d4ll231gqe6n719kla.png-w.jpg" height="180" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cai9u8ks169k191mrs0an9dmfa.png-w.jpg" height="180" width="300"></div></div></div></div><p align="center"><strong>Fig.7. Reawrd detials of our proposed algorithm</strong></p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://t1.aixinxi.net/o_1cai9us9e1hg61mhoq55nrn58sa.png-w.jpg" height="180" width="800"></div></div></div></div><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://t1.aixinxi.net/o_1cai9v9a610id1gh1fas1k8rgj6a.png-w.jpg" height="180" width="800"></div></div></div></div><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://t1.aixinxi.net/o_1cai9vmq3m2pt3812h41510a7na.png-w.jpg" height="180" width="800"></div></div></div></div><p align="center"><strong>Fig.8. Cumulative steps detials of our proposed algorithm</strong></p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 25%;"><img src="http://t1.aixinxi.net/o_1caia0epj1s7r136p1o59ffg1fv4a.png-w.jpg" height="160" width="245"></div><div class="group-picture-column" style="width: 25%;"><img src="http://t1.aixinxi.net/o_1caia14q9deekmi1a4d1mu41j92a.png-w.jpg" height="160" width="245"></div><div class="group-picture-column" style="width: 25%;"><img src="http://t1.aixinxi.net/o_1caia1kuh1rae1gio13e4vvr8jha.png-w.jpg" height="160" width="245"></div><div class="group-picture-column" style="width: 25%;"><img src="http://t1.aixinxi.net/o_1caia2eik7fp1gnb1bathhe1mn4a.png-w.jpg" height="160" width="245"></div></div></div></div><p align="center"><strong>Fig.9. Velocity and trackpos detials of our  proposed algorithm</strong></p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cbepms1f16flp4e1up1lqvolma.gif-w.jpg" height="160" width="240"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cbepnur11vkk1anq1dnlqpibe5a.gif-w.jpg" height="160" width="240"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://t1.aixinxi.net/o_1cbepoqnj11r41ec5vqb1gqq1vdua.gif-w.jpg" height="160" width="240"></div></div></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h1&gt;&lt;p&gt;In this paper, we propose a deep reinforcement
      
    
    </summary>
    
    
  </entry>
  
</feed>
