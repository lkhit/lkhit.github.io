<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Lucky&#39;s hoom</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-04-05T08:54:05.041Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Liu Kai</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>A Deep Reinforcement Learning Algorithm with Expert Demonstrations and Supervised Loss and its application into Autonomous Driving</title>
    <link href="http://yoursite.com/2018/03/29/TORCS/"/>
    <id>http://yoursite.com/2018/03/29/TORCS/</id>
    <published>2018-03-29T03:35:25.000Z</published>
    <updated>2018-04-05T08:54:05.041Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Deep Reinforcement learning (RL) is considered to be a strong machine learning algorithm which can be used to teach machines through interaction with the environment and learning from trial-error. However, the real-world application of RL is still restricted by a variety of reasons. The two most significant challenges of RL are the large exploration space and the difficulty to converge. Training RL agent with expert demonstration is technically an interesting way to accelerate the exploration process and increase the stability. In this work, we propose a continuous reinforcement learning method which combines Deep Deterministic Policy Gradient (DDPG) with expert demonstrations for intelligent decision making for autonomous driving. The proposed algorithm introduces supervised classification loss function for updating the actor networks. In addition, reward construction is combined to make the training process more stable and efficient. The proposed algorithm is tested on a popular autonomous driving simulator called TORCS. The experimental results show that our method not only improves training efficiency, but also improves learning stability.</p><p>The autonomous driving task can be viewed as Figure1.<br><img src="http://i1.bvimg.com/638842/db51ca0b3c8a93d0.png" alt="example image"></p><p align="center"><strong>Figure1. Autonomous driving framework</strong></p><p>The training track and testing track are shown as Figure2.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://i1.bvimg.com/638842/71221fdeccc3d4c8.png" height="300" width="350"></div><div class="group-picture-column" style="width: 50%;"><img src="http://i1.bvimg.com/638842/254e598739862178.png" height="300" width="350"></div></div></div></div><p align="center"><strong>Figure2. Training track(left) and test track(right)</strong></p><p>We record the training video of original DDPG algorithm in 200, 500, 1000 episode respectively, and the video is shown as Figure3.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i4.bvimg.com/638842/3b7af52f5dff256b.gif" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i4.bvimg.com/638842/319e6f133a909da0.gif" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i4.bvimg.com/638842/70632aa37b237c06.gif" height="250" width="300"></div></div></div></div><p align="center"><strong>Figure3. Training video of original DDPG</strong></p><p>For comparison experiment, we record the training video of DDPG with Expert Demonstrations algorithm in 200, 500, 1000 episode at the same time, and the video is shown as Figure4.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i4.bvimg.com/638842/ba3b51dd9a2930f3.gif" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i4.bvimg.com/638842/8887659e9223efbc.gif" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i4.bvimg.com/638842/56576f35e2622cd7.gif" height="250" width="300"></div></div></div></div><p align="center"><strong>Figure4. Training video of DDPG with Expert Demonstrations</strong></p><p>Meanwhile, we record the training video of DDPG with modified reward function in 200, 500, 1000 episode, and the video is shown as Figure5.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i4.bvimg.com/638842/1aecaa69fb94d824.gif" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i4.bvimg.com/638842/671c2ddbc39734ff.gif" height="250" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i4.bvimg.com/638842/df006d34d802b64b.gif" height="250" width="300"></div></div></div></div><p align="center"><strong>Figure5. Training video of DDPG with modifed reward</strong></p><p>We visualize the test process(CG1 and CG2 track), the results are shown as figure6 and figure7.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="http://i4.bvimg.com/638842/df006d34d802b64b.gif" height="300" width="350"></div><div class="group-picture-column" style="width: 50%;"><img src="http://i4.bvimg.com/638842/12d57afcac78e3b0.gif" height="300" width="350"></div></div></div></div><p align="center"><strong>Figure6. Agent test video in CG1 and CG2 track</strong></p><p>More comparison detials of our algorithm are shown as Figure7, Figure8, Figure9.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i1.bvimg.com/638842/5c0dea8e061ebf05.png" height="180" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i1.bvimg.com/638842/0690e7392f7c05e2.png" height="180" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i1.bvimg.com/638842/f4b244b61d480d29.png" height="180" width="300"></div></div></div></div><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i2.bvimg.com/638842/d55b54b22646c33e.png" height="180" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i1.bvimg.com/638842/bbeda50e55867d1a.png" height="180" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i2.bvimg.com/638842/156f9c3802f18b8e.png" height="180" width="300"></div></div></div></div><p align="center"><strong>Figure7. Reawrd detials of our algorithm</strong></p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://i4.bvimg.com/638842/49e3a7166662f9c3.png" height="180" width="800"></div></div></div></div><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://i4.bvimg.com/638842/167b0b8ab35a8531.png" height="180" width="800"></div></div></div></div><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://i4.bvimg.com/638842/334f3706fa9b5f7e.png" height="180" width="800"></div></div></div></div><p align="center"><strong>Figure8. Cumulative steps detials of our algorithm</strong></p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i1.bvimg.com/638842/5c0dea8e061ebf05.png" height="180" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i1.bvimg.com/638842/0690e7392f7c05e2.png" height="180" width="300"></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://i1.bvimg.com/638842/f4b244b61d480d29.png" height="180" width="300"></div></div></div></div><p align="center"><strong>Figure9. Velocity and trackpos detials of our algorithm</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h1&gt;&lt;p&gt;Deep Reinforcement learning (RL) is considered
      
    
    </summary>
    
    
  </entry>
  
</feed>
